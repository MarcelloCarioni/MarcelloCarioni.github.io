---
layout: archive
title: "SPARGO: Exploring and Exploiting the Geometric Landscape of Infinite-Dimensional Sparse Optimization"
permalink: /spargo/
author_profile: true
---

<br>

<br>


<img src="{{ site.baseurl }}/images/nwo_logo.jpg" alt="My image" width="150" align="middle">
<img src="{{ site.baseurl }}/images/logo-stacked.png" alt="My image" width="200" align="center">
<img src="{{ site.baseurl }}/images/mia.png" alt="My image" width="250" align="right">

<br>

<br>

<br>


<b> Summary: </b> 

<em> It is becoming clear that our society is overwhelmed by a growing flood of information, much of which is either redundant or unreliable. 
Even in our daily lives, we constantly face the challenge of filtering content from a variety of media. One way to tackle this challenge is through 
compression methods, which are already widely applied to finite-dimensional data. However, when it comes to infinite-dimensional structures which are 
often used to describe modern data, such methods remain unexplored. SPARGO aims to bridge this gap by developing a new framework for 
infinite-dimensional compression </em>

<br>

<img src="{{ site.baseurl }}/images/pppp.png" alt="My image" width="500" align="center">

<br>

<b> SPARGO objectives: </b> 

<br>

<ul>
  <li> <b> <p style="color:DodgerBlue;"> OB1: Sparse stability in infinite dimension.  </p> </b> Is infinite-dimensional <b> sparsity preserved under perturbations </b> of the optimization problem?  <b> OB1  </b> addresses this
question by investigating how the ability to solve an optimization problem using only few building blocks is stable
with respect to various types of perturbations.  </li>
  <li> <b> <p style="color:MediumSeaGreen;"> OB2: Analysis of Atomic Gradient Descents approaches. </p> </b> How can we use the sparse structure of infinite-dimensional optimization problems to  <b> design algorithms</b>? The
characterization of the sparse building blocks of a given optimization problem is instrumental to design algorithms
that use such structure in the iterate.  <b> OB2  </b> introduces a new class of infinite-dimensional sparse optimization algorithms
named  <b> Atomic Gradient Descent methods  </b> that is built on such principle. </li>
</ul>

