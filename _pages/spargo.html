---
layout: archive
title: "SPARGO: Exploring and Exploiting the Geometric Landscape of Infinite-Dimensional Sparse Optimization"
permalink: /spargo/
author_profile: true
---





It is becoming clear that our society is overwhelmed by a growing flood of information, much of which is either redundant or unreliable. 
Even in our daily lives, we constantly face the challenge of filtering content from a variety of media. One way to tackle this challenge is through 
compression methods, which are already widely applied to finite-dimensional data. However, when it comes to infinite-dimensional structures which are 
often used to describe modern data, such methods remain unexplored. 
<br>
<b> SPARGO aims to bridge this gap by developing a new framework for infinite-dimensional compression </b>  

<br>


<br>

<div style="text-align: center;">

<img src="{{ site.baseurl }}/images/pppp.png" alt="My image" width="700" align="center">

</div>  
<br>



The development of compression and sparse methods in infinite dimension has been limited by the lack of a suitable general  <b> notion of sparsity  </b> that would provide information
on how we should <b> optimally represents infinite-dimensional data.</b> It has been generally accepted that compressing
means optimally representing data using a collection of <b> few simple building blocks.</b> However, no general recipe has been given to understand the reason why certain building blocks should be used instead of
others.<b> SPARGO advocates that the sparsity features of a given infinite-dimensional model are determined by its geometric properties. </b> It thus aims to characterize building blocks and provide sparse representations by
studying the geometric structure of an optimization problem. Such sparse, or compressed, representations will be then instrumental to provide <b> reconstruction guarantees </b> as well as to <b> design compression algorithms </b> that take advantage of optimal sparse representations. 

<br>

<br>

<b> SPARGO aims to achieve the following objectives: </b> 

<br>

<span>&#8226;</span> <b> <p style="color:DodgerBlue;"> Sparse stability in infinite dimension.</p></b>Is infinite-dimensional <b> sparsity preserved under perturbations </b> of the optimization problem?  <b> SPARGO  </b> addresses this
question by investigating how the ability to solve an optimization problem using only few building blocks is stable
with respect to various types of perturbations.  
<br>
 <span>&#8226;</span>  <b> <p style="color:MediumSeaGreen;"> Analysis of Atomic Gradient Descents approaches. </p> </b> How can we use the sparse structure of infinite-dimensional optimization problems to  <b> design algorithms</b>? The
characterization of the sparse building blocks of a given optimization problem is instrumental to design algorithms
that use such structure in the iterate.  <b> SPARGO  </b> introduces a new class of infinite-dimensional sparse optimization algorithms
named  <b> Atomic Gradient Descent methods  </b> that is built on such principle. 




<br>


<div style="text-align: center;">

<img src="{{ site.baseurl }}/images/ssss.png" alt="My image" width="800" align="center">

</div>  
<br>

<br>


<b> Bibliography: </b>
<br>
<br>
<span>&#8226;</span> <em> Kristian Bredies, Marcello Carioni. </em>  <b> Sparsity of solutions for variational inverse problems with finite-dimensional data.</b> Calculus of Variations and Partial Differential Equations (2020)
<br>
<span>&#8226;</span> <em> Marcello Carioni, Leonardo Del Grande. </em> <b>  A general theory for exact sparse representation recovery in convex optimization. </b>  (2023)
<br>
<span>&#8226;</span> <em> Marcello Carioni, José Iglesias, Daniel Walter. </em>  <b>  Extremal points and sparse optimization for generalized Kantorovich–Rubinstein norms. </b> Foundations of Computational Mathematics (2024)
<br>
<span>&#8226;</span> <em> Kristian Bredies, Marcello Carioni, Silvio Fanzon, Daniel Walter. </em> <b>  Asymptotic linear convergence of fully-corrective generalized conditional gradient methods.</b> Mathematical Programming  (2025)
<br>
<span>&#8226;</span> <em> Kristian Bredies, Marcello Carioni, Silvio Fanzon, Francisco Romero. </em>  <b>  A generalized conditional gradient method for dynamic inverse problems with optimal transport regularization.</b> Foundations of Computational Mathematics (2024)
<br>
<span>&#8226;</span> <em> Francesca Bartolucci, Marcello Carioni, José Iglesias, Yury Korolev, Emanuele Naldi, Stefano Vigogna. </em>  <b>  A Lipschitz spaces view of infinitely wide shallow neural networks.</b> SIAM Mathematical Analysis (2024)



  
<br>
<br>

<div style="text-align: center;">

<img src="{{ site.baseurl }}/images/nwo_logo.jpg" alt="My image" width="150" align="middle">
<img src="{{ site.baseurl }}/images/logo-stacked.png" alt="My image" width="200" align="center">
<img src="{{ site.baseurl }}/images/mia.png" alt="My image" width="250">


</div>  
<br>

